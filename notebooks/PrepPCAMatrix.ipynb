{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cda321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from utils import parse_tfrecord\n",
    "import faiss \n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d61821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filenames = [f\"./one_percent_embeddings/a2o_sample_embeddings-{i:05}-of-00374\" for i in range(0, 374)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f9f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 374/374 [08:12<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data records: 14412192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "metadata = []\n",
    "count = 0\n",
    "for dataset_filename in tqdm(dataset_filenames): \n",
    "    embeddings =[]\n",
    "    metadata = []\n",
    "    raw_dataset = tf.data.TFRecordDataset(dataset_filename)\n",
    "    for timestamp_s, filename, embedding, embedding_shape in raw_dataset.map(parse_tfrecord).as_numpy_iterator():\n",
    "        [(\n",
    "            site_id, \n",
    "            file_datetime, \n",
    "            timezone, \n",
    "            site_name, \n",
    "            subsite_name, \n",
    "            file_seq_id\n",
    "        )] = re.findall(\n",
    "            # I'm quite proud of myself for this regex, but if anyone can see \n",
    "            # a way to simplify it, please let me know!\n",
    "            r\"site_(?P<site_id>\\d{4})\\/(?P<datetime>\\d{8}T\\d{6})(?P<timezone>(?:\\+\\d{4})|Z)_(?P<site_name>(?:\\w*|-)*)-(?P<subsite_name>(?:Wet|Dry)-(?:A|B))_(?P<file_seq_id>\\d*).flac\",\n",
    "            filename.decode(\"utf-8\")\n",
    "        )\n",
    "        \n",
    "        # Some files have just \"Z\" as timezone, assume UTC in this case\n",
    "        timezone = \"+0000\" if timezone == \"Z\" else timezone\n",
    "        file_datetime = datetime.datetime.strptime(f\"{file_datetime}{timezone}\", \"%Y%m%dT%H%M%S%z\")\n",
    "        midnight = file_datetime.replace(hour=0, minute=0, second=0)\n",
    "        file_offset_since_midnight = (file_datetime - midnight).seconds\n",
    "        \n",
    "        # `embedding` is a 3D array with Dims [12,1,1280]\n",
    "        # We loop over the first dimension to \"flatten\" \n",
    "        # the 12 emebddings per minute\n",
    "        # and extract the single channel (2nd dimension). \n",
    "        # We add each of the 12 embeddings as their own record\n",
    "        for i, _embedding in enumerate(embedding[:,0]):\n",
    "\n",
    "            #embeddings.append(_embedding)\n",
    "            count +=1\n",
    "            metadata.append({\n",
    "                \"file_timestamp\": int(file_datetime.timestamp()),\n",
    "                \"file_seconds_since_midnight\": file_offset_since_midnight,\n",
    "                \"recording_offset_in_file\": int(timestamp_s + (5*i)), \n",
    "                \"site_id\": site_id, \n",
    "                \"site_name\": site_name, \n",
    "                \"subsite_name\": subsite_name, \n",
    "                \"file_seq_id\": int(file_seq_id),\n",
    "                \"filename\": filename.decode(\"utf-8\")\n",
    "            })\n",
    "\n",
    "    with open(f\"./one_percent_embeddings_metadata/{dataset_filename.split('/')[-1]}.json\", \"w\") as f: \n",
    "        f.write(json.dumps(metadata))\n",
    "    #np.save(f\"./one_percent_embeddings_numpy/{dataset_filename.split('/')[-1]}.npy\", embeddings)\n",
    "print(f\"Total number of data records: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c595fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filenames = [f\"./one_percent_embeddings_metadata/a2o_sample_embeddings-{i:05}-of-00374.json\" for i in range(0, 374)]\n",
    "embedding_numpy_filenames = [f\"./one_percent_embeddings_numpy/a2o_sample_embeddings-{i:05}-of-00374.npy\" for i in range(0, 374)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dee497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train PCA matrix on subset of total matrix\n",
    "# 0.2 chosen arbitrarily\n",
    "pca_training_sample_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "training_set = []\n",
    "for embedding_file in tqdm(embedding_numpy_filenames): \n",
    "    embeddings = np.load(embedding_file)\n",
    "    rand_indexes = np.random.randint(low=0, high=len(embeddings), size=int(pca_training_sample_size * len(embeddings)))\n",
    "    print(rand_indexes)\n",
    "    subset = embeddings[rand_indexes]\n",
    "    training_set.extend(list(subset))\n",
    "\n",
    "training_set = np.array(training_set)\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fadd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mat = faiss.PCAMatrix(1280, 256)\n",
    "mat.train(training_set)\n",
    "faiss.write_VectorTransform(mat, \"1280_to_256_dimensionality_reduction.pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca_matrix = faiss.read_VectorTransform(\"1280_to_256_dimensionality_reduction.pca\")\n",
    "for embedding_file in tqdm(embedding_numpy_filenames): \n",
    "    embeddings = np.load(embedding_file)\n",
    "    reduced_embeddings = pca_matrix.apply(embeddings)\n",
    "    np.save(embedding_file.replace('_numpy', '_numpy_reduced'), reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9cc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for metadata_file in metadata_filenames: \n",
    "    with open(metadata_file, \"r\") as f: \n",
    "        _metadata = json.loads(f.read())\n",
    "        metadata.extend(_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2b947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086f015a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_timestamp</th>\n",
       "      <th>file_datetime</th>\n",
       "      <th>file_seconds_since_midnight</th>\n",
       "      <th>recording_offset_in_file</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>subsite_name</th>\n",
       "      <th>file_seq_id</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.603584e+09</td>\n",
       "      <td>2020-10-25T11:00:00+11:00</td>\n",
       "      <td>39600</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>0302</td>\n",
       "      <td>Cape-Barren-Island</td>\n",
       "      <td>Dry-B</td>\n",
       "      <td>810300</td>\n",
       "      <td>site_0302/20201025T110000+1100_Cape-Barren-Isl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.603584e+09</td>\n",
       "      <td>2020-10-25T11:00:00+11:00</td>\n",
       "      <td>39600</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>0302</td>\n",
       "      <td>Cape-Barren-Island</td>\n",
       "      <td>Dry-B</td>\n",
       "      <td>810300</td>\n",
       "      <td>site_0302/20201025T110000+1100_Cape-Barren-Isl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.603584e+09</td>\n",
       "      <td>2020-10-25T11:00:00+11:00</td>\n",
       "      <td>39600</td>\n",
       "      <td>3310.0</td>\n",
       "      <td>0302</td>\n",
       "      <td>Cape-Barren-Island</td>\n",
       "      <td>Dry-B</td>\n",
       "      <td>810300</td>\n",
       "      <td>site_0302/20201025T110000+1100_Cape-Barren-Isl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.603584e+09</td>\n",
       "      <td>2020-10-25T11:00:00+11:00</td>\n",
       "      <td>39600</td>\n",
       "      <td>3315.0</td>\n",
       "      <td>0302</td>\n",
       "      <td>Cape-Barren-Island</td>\n",
       "      <td>Dry-B</td>\n",
       "      <td>810300</td>\n",
       "      <td>site_0302/20201025T110000+1100_Cape-Barren-Isl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.603584e+09</td>\n",
       "      <td>2020-10-25T11:00:00+11:00</td>\n",
       "      <td>39600</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>0302</td>\n",
       "      <td>Cape-Barren-Island</td>\n",
       "      <td>Dry-B</td>\n",
       "      <td>810300</td>\n",
       "      <td>site_0302/20201025T110000+1100_Cape-Barren-Isl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_timestamp              file_datetime  file_seconds_since_midnight   \n",
       "0    1.603584e+09  2020-10-25T11:00:00+11:00                        39600  \\\n",
       "1    1.603584e+09  2020-10-25T11:00:00+11:00                        39600   \n",
       "2    1.603584e+09  2020-10-25T11:00:00+11:00                        39600   \n",
       "3    1.603584e+09  2020-10-25T11:00:00+11:00                        39600   \n",
       "4    1.603584e+09  2020-10-25T11:00:00+11:00                        39600   \n",
       "\n",
       "   recording_offset_in_file site_id           site_name subsite_name   \n",
       "0                    3300.0    0302  Cape-Barren-Island        Dry-B  \\\n",
       "1                    3305.0    0302  Cape-Barren-Island        Dry-B   \n",
       "2                    3310.0    0302  Cape-Barren-Island        Dry-B   \n",
       "3                    3315.0    0302  Cape-Barren-Island        Dry-B   \n",
       "4                    3320.0    0302  Cape-Barren-Island        Dry-B   \n",
       "\n",
       "  file_seq_id                                           filename  \n",
       "0      810300  site_0302/20201025T110000+1100_Cape-Barren-Isl...  \n",
       "1      810300  site_0302/20201025T110000+1100_Cape-Barren-Isl...  \n",
       "2      810300  site_0302/20201025T110000+1100_Cape-Barren-Isl...  \n",
       "3      810300  site_0302/20201025T110000+1100_Cape-Barren-Isl...  \n",
       "4      810300  site_0302/20201025T110000+1100_Cape-Barren-Isl...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de94902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filename.str.len().min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioacoustics",
   "language": "python",
   "name": "bioacoustics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
