{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda4683a",
   "metadata": {},
   "source": [
    "# Setup Milvus collection and ingest embeddings + metadata\n",
    "This notebook will walk through the steps necessary create a collection in Milvus for the one percent data sample, and ingest the metadata and reduced embeddings.\n",
    "\n",
    "### Pre-requisites: \n",
    "You will need the reduced embeddings generated in the pervious notebook: `2_train_apply_pca.ipynb`\n",
    "\n",
    "**Author:** Leo Thomas - leo@developmentseed.org\\\n",
    "**Last updated:** 2023/06/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0692bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import concurrent.futures\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e547b",
   "metadata": {},
   "source": [
    "### 1.0. Instantiate a connection through kubectl to the Milvus instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363d5549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connections:  [('default', <pymilvus.client.grpc_handler.GrpcHandler object at 0x10cbf1c40>)]\n"
     ]
    }
   ],
   "source": [
    "# Connecte to remote Milvus instance by using kubectl port forwarding: \n",
    "# gcloud auth login\n",
    "# gcloud container clusters get-credentials bioacoustics-devseed-staging-cluster --region=us-central1-f  --project dulcet-clock-385511\n",
    "# kubectl port-forward service/milvus 9091:9091 & kubectl port-forward service/milvus 19530:19530 &\n",
    "\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 19530\n",
    "connections.connect(host=HOST, port=PORT)\n",
    "print(\"Connections: \", connections.list_connections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1496c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"a2o_bioacoustics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e8e08",
   "metadata": {},
   "source": [
    "### 2.0. Drop collection if exists. \n",
    "\n",
    "# WARNING: this will delete whichever collection is deployed proceed with CAUTION\n",
    "The rest of the notebook will walk throught the steps to load data into the Milvus instance and create the necessary indexes, but the process may take quite a while, and requires uploading ~20Gb of reduced embeddings and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop collection if exists\n",
    "if utility.has_collection(COLLECTION_NAME): \n",
    "    print(\"Collection exists, dropping...\")\n",
    "    collection = Collection(COLLECTION_NAME)\n",
    "    collection.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53564d",
   "metadata": {},
   "source": [
    "### 3.0. Define schema for the new collection and create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43248251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define collection fields\n",
    "id_field = FieldSchema(\n",
    "    name=\"id\", dtype=DataType.INT64, descrition=\"primary field\", is_primary=True, auto_id=True\n",
    ")\n",
    "\n",
    "embedding_field = FieldSchema(\n",
    "    name=\"embedding\", \n",
    "    dtype=DataType.FLOAT_VECTOR, \n",
    "    description=\"Float32 vector with dim 256 (reduced using PCA from origina vector dim:1280)\", \n",
    "    dim=256,\n",
    "    is_primary=False\n",
    ")\n",
    "\n",
    "file_timestamp_field = FieldSchema(\n",
    "    name=\"file_timestamp\", \n",
    "    dtype=DataType.INT64, \n",
    "    description=\"UTC File timestamp (in seconds since 1970-01-01T00:00:00)\"\n",
    ")\n",
    "\n",
    "file_seconds_since_midnight_field = FieldSchema(\n",
    "    name=\"file_seconds_since_midnight\", \n",
    "    dtype=DataType.INT64, \n",
    "    description=\"Number of seconds since 00h00 (within same timezone)\"\n",
    ")\n",
    "\n",
    "clip_offset_in_file_field = FieldSchema(\n",
    "    name=\"clip_offset_in_file\", \n",
    "    dtype=DataType.INT64, \n",
    "    description=\"Offset (in seconds) from start of file where embedding window starts\"\n",
    ")\n",
    "\n",
    "# max len found in 1% sample = 4\n",
    "site_id_field = FieldSchema(\n",
    "    name=\"site_id\", dtype=DataType.VARCHAR, description=\"Site ID\", max_length=8\n",
    ")\n",
    "\n",
    "# max len found in 1% sample = 50\n",
    "site_name_field = FieldSchema(\n",
    "    name=\"site_name\", dtype=DataType.VARCHAR, description=\"Site name\", max_length=100 \n",
    ")\n",
    "\n",
    "# (one of: Dry-A, Dry-B, Wet-A, Wet-B)\n",
    "subsite_name_field = FieldSchema(\n",
    "    name=\"subsite_name\", dtype=DataType.VARCHAR, description=\"Subsite name\", max_length=5 \n",
    ")\n",
    "\n",
    "# sequence id can be converted to int\n",
    "file_seq_id_field = FieldSchema(\n",
    "    name=\"file_seq_id\", dtype=DataType.INT64, description=\"File sequence ID\", \n",
    ")\n",
    "filename_field = FieldSchema(\n",
    "    name=\"filename\", dtype=DataType.VARCHAR, max_length=500\n",
    ")\n",
    "\n",
    "schema = CollectionSchema(\n",
    "    fields=[\n",
    "        id_field,\n",
    "        embedding_field, \n",
    "        file_timestamp_field,\n",
    "        file_seconds_since_midnight_field,\n",
    "        clip_offset_in_file_field, \n",
    "        site_id_field, \n",
    "        site_name_field, \n",
    "        subsite_name_field, \n",
    "        file_seq_id_field, \n",
    "        filename_field\n",
    "    ], \n",
    "    description=\"Collection for A20 bird call embeddings\"\n",
    ")\n",
    "\n",
    "\n",
    "collection = Collection(\n",
    "    name=COLLECTION_NAME, \n",
    "    # instantiate colleciton with no data\n",
    "    data=None,\n",
    "    schema=schema, \n",
    "    # Set TTL to 0 to disable data expiration\n",
    "    properties={\"collection.ttl.seconds\": 0}\n",
    ")\n",
    "\n",
    "print(f\"Collections: {utility.list_collections()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260b2da",
   "metadata": {},
   "source": [
    "### 4.0. Create index on the embedding field in the collection\n",
    "The index will greatly reduce search time and further reduce memory footprint, at the cost of a slight decrease in accuracy. See the index evaluation notebook for a thorough comparison of the various indexing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating new index: IVF_SQ8 with params nlist:4096\")\n",
    "# create new index: \n",
    "index_params = {\n",
    "    \"index_type\": \"IVF_SQ8\",\n",
    "    \"params\":{\"nlist\": 4096},\n",
    "    \"metric_type\": \"L2\"\n",
    "}\n",
    "\n",
    "collection.create_index(\"embedding\", index_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6400b54",
   "metadata": {},
   "source": [
    "### 5.0. Define constants\n",
    "The `BATCH_SIZE` parameter will defined how many data entities are loaded at once. Very large batches can overload the machine's memory and very small batches can lead to longer ingestion times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1000\n",
    "FIELD_NAMES = (\n",
    "    \"embedding\",\n",
    "    \"file_timestamp\",\n",
    "    \"file_seconds_since_midnight\",\n",
    "    \"recording_offset_in_file\",\n",
    "    \"site_id\",\n",
    "    \"site_name\",\n",
    "    \"subsite_name\", \n",
    "    \"file_seq_id\", \n",
    "    \"filename\"\n",
    ")\n",
    "DATA_DIR = os.path.abspath(\"./one_percent_data_sample\")\n",
    "METADATA_DIR = os.path.join(DATA_DIR, \"metadata\")\n",
    "REDUCED_EMBEDDINGS_DIR = os.path.join(DATA_DIR, \"reduced_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb3cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to batch data: \n",
    "def split_into_batches(data, n=10_000): \n",
    "    for i in range(0, len(data), n):\n",
    "        yield data[i:i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81ebbd",
   "metadata": {},
   "source": [
    "### 6.0. Load the data\n",
    "The insert operation expects a list of lists, where each sub-list contains the values corresponding to one field: \n",
    "eg: \n",
    "```python\n",
    "[\n",
    "    [ embedding_1, embedding_2, ..., embedding_n],\n",
    "    [ file_timestamp_1, file_timestamp_2, ..., file_timestamp_n],\n",
    "    ...\n",
    "    [filename_1, filename_2, ..., filename_3]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf13deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def load_data(metadata_file): \n",
    "    \n",
    "    embeddings_filename = metadata_file.split(\"/\")[-1].replace(\".json\", \".npy\")\n",
    "    embeddings_file = os.path.join(REDUCED_EMBEDDINGS_DIR, embeddings_filename)\n",
    "    _embeddings = np.load(embeddings_file)\n",
    "    \n",
    "    _metadata = None\n",
    "    with open(metadata_file, \"r\") as f: \n",
    "        _metadata = json.loads(f.read())\n",
    "        \n",
    "    assert len(_embeddings) == len(_metadata)\n",
    "    \n",
    "    data = [\n",
    "        {\"embedding\": _embeddings[i], **_metadata[i]} \n",
    "        for i in range(len(_metadata)) \n",
    "    ]\n",
    "        \n",
    "    collection = Collection(COLLECTION_NAME)\n",
    "    \n",
    "    for batch in split_into_batches(data, BATCH_SIZE): \n",
    "        \n",
    "        collection.insert(\n",
    "            [[_data[fieldname] for _data in batch] for fieldname in FIELD_NAMES]\n",
    "        )\n",
    "        collection.flush()\n",
    "\n",
    "    return len(_embeddings)\n",
    "\n",
    "metadata_files = [\n",
    "    os.path.join(METADATA_DIR, file) \n",
    "    for file in os.listdir(METADATA_DIR) \n",
    "    if os.path.isfile(os.path.join(METADATA_DIR, file))\n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(\n",
    "        tqdm(\n",
    "            executor.map(\n",
    "                lambda x: load_data(x),\n",
    "                metadata_files\n",
    "            ), \n",
    "            total=len(metadata_files) \n",
    "        )\n",
    "    ) \n",
    "\n",
    "print(f\"Collection {COLLECTION_NAME} currently loaded with {collection.num_entities} entities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioacoustics",
   "language": "python",
   "name": "bioacoustics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
