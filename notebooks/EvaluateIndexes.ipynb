{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6a4954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 13:37:31.981142: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from milvus import default_server\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "from utils import extract_sample_data, split_into_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f3290f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "Milvus not startd in 30.0 seconds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m default_server\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# startup milvus server\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdefault_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Set up milvus server\u001b[39;00m\n\u001b[1;32m      8\u001b[0m HOST \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/development-seed/bioacoustics/bioacoustics-api/venv/lib/python3.8/site-packages/milvus/__init__.py:424\u001b[0m, in \u001b[0;36mMilvusServer.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(old_pwd)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_started:\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_started\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_banner()\n",
      "File \u001b[0;32m~/development-seed/bioacoustics/bioacoustics-api/venv/lib/python3.8/site-packages/milvus/__init__.py:388\u001b[0m, in \u001b[0;36mMilvusServer.wait_started\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    386\u001b[0m         sleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning:\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMilvus not startd in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMilvus server already stopped\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Milvus not startd in 30.0 seconds"
     ]
    }
   ],
   "source": [
    "# clean up any lingering data in milvus server\n",
    "default_server.cleanup()\n",
    "\n",
    "# startup milvus server\n",
    "default_server.start()\n",
    "\n",
    "# Set up milvus server\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = default_server.listen_port\n",
    "\n",
    "# Spin up server (remember to close after)\n",
    "connections.connect(host=HOST, port=PORT)\n",
    "print(\"Connections: \", connections.list_connections())\n",
    "\n",
    "# set up collection\n",
    "COLLECTION_NAME = \"a2o_bioacoustics\"\n",
    "\n",
    "# Drop collection if exists\n",
    "if utility.has_collection(COLLECTION_NAME): \n",
    "    collection = Collection(COLLECTION_NAME)\n",
    "    collection.drop()\n",
    "\n",
    "# define collection fields\n",
    "id_field = FieldSchema(\n",
    "    name=\"id\", \n",
    "    dtype=DataType.INT64, \n",
    "    descrition=\"primary field\", \n",
    "    is_primary=True, \n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "embedding_field = FieldSchema(\n",
    "    name=\"embedding\", \n",
    "    dtype=DataType.FLOAT_VECTOR, \n",
    "    description=\"Float32 vector with dim 1280\", \n",
    "    dim=1280,\n",
    "    is_primary=False\n",
    ")\n",
    "file_timestamp_field = FieldSchema(\n",
    "    name=\"file_timestamp\", \n",
    "    dtype=DataType.INT64, \n",
    "    description=\"File timestamp (in seconds since 1970-01-01T00:00:00)\"\n",
    ")\n",
    "offset_field = FieldSchema(\n",
    "    name=\"offset\", \n",
    "    dtype=DataType.INT64, \n",
    "    description=\"Offset (in seconds) from start of file where embedding window starts\"\n",
    ")\n",
    "site_id_field = FieldSchema(\n",
    "    name=\"site_id\", \n",
    "    dtype=DataType.INT64, \n",
    "    description=\"Site ID\", \n",
    ")\n",
    "site_name_field = FieldSchema(\n",
    "    name=\"site_name\", \n",
    "    dtype=DataType.VARCHAR, \n",
    "    description=\"Site name\", \n",
    "    max_length=1000\n",
    ")\n",
    "subsite_name_field = FieldSchema(\n",
    "    name=\"subsite_name\", \n",
    "    dtype=DataType.VARCHAR, \n",
    "    description=\"Subsite name\", \n",
    "    max_length=1000\n",
    ")\n",
    "file_seq_id_field = FieldSchema(\n",
    "    name=\"file_seq_id\", \n",
    "    dtype=DataType.INT64, \n",
    "    description=\"File sequence ID\", \n",
    ")\n",
    "filename_field = FieldSchema(\n",
    "    name=\"filename\", \n",
    "    dtype=DataType.VARCHAR, \n",
    "    max_length=1000\n",
    ")\n",
    "\n",
    "schema = CollectionSchema(\n",
    "    fields=[\n",
    "        id_field,\n",
    "        embedding_field, \n",
    "        file_timestamp_field,\n",
    "        offset_field, \n",
    "        site_id_field, \n",
    "        site_name_field, \n",
    "        subsite_name_field, \n",
    "        file_seq_id_field, \n",
    "        filename_field\n",
    "    ], \n",
    "    description=\"Collection for searching A20 bird embeddings\"\n",
    ")\n",
    "collection = Collection(\n",
    "    name=COLLECTION_NAME, \n",
    "    data=None,\n",
    "    schema=schema, \n",
    "    # Set TTL to 0 to disable\n",
    "    properties={\"collection.ttl.seconds\": 0}\n",
    ")\n",
    "print(f\"Collections: {utility.list_collections()}\")\n",
    "print(f\"Collection {COLLECTION_NAME} instantiated with {collection.num_entities} entities\")\n",
    "\n",
    "data = extract_sample_data()\n",
    "    \n",
    "# split data into batches of 10_000 for insertion into Milvus collection\n",
    "# TODO: find documentation on why this is necessary, I did this to try \n",
    "# to get around the kernel dying when trying to insert the entire \n",
    "# collection at once\n",
    "for _batch in split_into_batches(data): \n",
    "\n",
    "    #insert \n",
    "    collection.insert(\n",
    "        [\n",
    "            [_data[fieldname] for _data in _batch] \n",
    "            for fieldname in (\n",
    "                \"embedding\",\n",
    "                \"file_timestamp\",\n",
    "                \"offset\",\n",
    "                \"site_id\",\n",
    "                \"site_name\",\n",
    "                \"subsite_name\", \n",
    "                \"file_seq_id\", \n",
    "                \"filename\"\n",
    "            )\n",
    "        ]\n",
    "    ) \n",
    "\n",
    "collection.flush()\n",
    "print(f\"Collection {COLLECTION_NAME} currently loaded with {collection.num_entities} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_params = {\n",
    "    \"index_type\": \"FLAT\",\n",
    "    \"params\":{},\n",
    "    \"metric_type\": \"L2\"\n",
    "}\n",
    "collection.create_index(\"embedding\", index_params)\n",
    "print(f\"Created index {collection.index().params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ebae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select a set of vectors to use as search\n",
    "# vectors to compare the different indexing strategies\n",
    "search_vectors = [data[i][\"embedding\"] for i in np.random.choice(range(len(data)), size=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f437c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "collection.load()\n",
    "\n",
    "search_param = {\n",
    "    \"data\": search_vectors,\n",
    "    \"anns_field\": \"embedding\",\n",
    "    \"param\": {\"metric_type\": \"L2\", \"params\": {}},\n",
    "    \"limit\": 100,\n",
    "}\n",
    "\n",
    "reference_results = collection.search(**search_param)\n",
    "collection.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result set is a 2D list with dims (100,100). \n",
    "# For each of the 100 input vectors, the inner list contains the 100\n",
    "# \"closest\" vectors\n",
    "print([v[0:5] for v in reference_results[0:5]])\n",
    "\n",
    "# the result set is a custom Milvus type:\n",
    "print(f\"{type(reference_results)}[{type(reference_results[0])}[{type(reference_results[0][0])}]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_recall(search, reference, at=100):\n",
    "    return len([i for i in search.ids[:at] if i in reference.ids[:at]])/at\n",
    "    \n",
    "# returns average recall@1, recall @10 and recall@100 \n",
    "# for each search result (compared to the reference result\n",
    "# set above)\n",
    "def score(search_results):\n",
    "    scores = [\n",
    "        [score_recall(search,ref,at=i) \n",
    "        for i in (1,10,100)]\n",
    "        # the search results class should be iterable, however something \n",
    "        # was affecting the orders of the ids when zipping the search\n",
    "        # result class, so I convert them first to a List[Hits].\n",
    "        for search, ref in zip(list(search_results), list(reference_results))\n",
    "    ]\n",
    "    return np.mean(np.array(scores), axis=0)\n",
    "\n",
    "# Assert scoring method works by scoring the \n",
    "# reference set against itself.\n",
    "score(reference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434aa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_index(index_name, index_params, search_params):\n",
    "    if len(collection.indexes): \n",
    "        print(f\"Dropping current index on field: {collection.index().field_name} -> {collection.index().params}\")\n",
    "        collection.drop_index()\n",
    "        print(f\"Indexes remaining after dropping: {collection.indexes}\")\n",
    "    \n",
    "    print(f\"Creating new index: {index_name} with params {index_params}\")\n",
    "    # create new index: \n",
    "    index_params = {\n",
    "        \"index_type\": index_name,\n",
    "        \"params\":index_params,\n",
    "        \"metric_type\": \"L2\"\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    collection.create_index(\"embedding\", index_params)\n",
    "    index_build_time = round((time.time()-start), 2)\n",
    "    \n",
    "    start = time.time()\n",
    "    collection.load()\n",
    "    print(f\"Loading collection took {round((time.time()-start), 2)} seconds\")\n",
    "    \n",
    "    search_param = {\n",
    "        \"data\": search_vectors,\n",
    "        \"anns_field\": \"embedding\",\n",
    "        \"params\": search_params,\n",
    "        \"limit\": 100,\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    search_results = collection.search(**search_param)\n",
    "    search_time = round((time.time()-start), 2)\n",
    "    \n",
    "    collection.release()\n",
    "    recall_score = score(search_results)\n",
    "    \n",
    "    return {\n",
    "        \"index_build_time\": index_build_time, \n",
    "        \"search_time\": search_time, \n",
    "        \"search_scores\":{\n",
    "            \"recall@1\": recall_score[0],\n",
    "            \"recall@10\": recall_score[1],\n",
    "            \"recall@100\": recall_score[2]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f09b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate \n",
    "ivf_flat_eval = evaluate_index(\n",
    "    index_name=\"IVF_FLAT\", \n",
    "    index_params={\"nlist\": 1024}, \n",
    "    search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 16}}\n",
    ")\n",
    "ivf_flat_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index IVF_SQ8 - good mix of improved speed + reduced memory footprint\n",
    "# NOTE: further investigate alternative indexing strategies: \n",
    "# HSNW/ANNOY for search time reduction\n",
    "# Product Quantization and PCA dimensionality reduction to reduce memory footprint\n",
    "ivf_sq8_eval = evaluate_index(\n",
    "    index_name=\"IVF_SQ8\", \n",
    "    index_params={\"nlist\": 1024}, \n",
    "    search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 16}}\n",
    ")\n",
    "ivf_sq8_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a23f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivf_pq4_eval = evaluate_index(\n",
    "    index_name=\"IVF_PQ\", \n",
    "    index_params={\"nlist\":1024, \"m\": 128, \"nbits\":8}, \n",
    "    search_params={\"metric_type\": \"L2\", \"params\": {\"nprobe\":16}}\n",
    ")\n",
    "ivf_pq4_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps: \n",
    "# - include data consumption (ram/disk) for each indexing stratgy\n",
    "# - include various options for building ivf_pq4 index (m/nbits)\n",
    "# - Add PCA pre-computation step to index evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioacoustics",
   "language": "python",
   "name": "bioacoustics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
