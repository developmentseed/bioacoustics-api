{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90faca24",
   "metadata": {},
   "source": [
    "# Train and apply PCA dimensionality reduction to the sample data\n",
    "This notebook will walk through the steps necessary to train and apply Principle Component Analysis (PCA) dimensionality reduction to the sample data embeddings in order to reduce the memory footprint of the Milvus data (or any vector database used to host the embeddings). PCA works by identifying the N dimensions with the largest co-variance (ie: the N least \"noisy\" dimensions) and only keeping those N dimensions accross all of the vectors. This allows us to signficantly reduce the size of the dataset while maintaining a very close approximation of the original vectors. We will use the faiss library to train and apply the PCA matrix for convenience.\n",
    "\n",
    "Note: once the PCA dimesnionality matrix has been trained and applied to the embeddings, the same PCA dimensionality reduction will have to be applied to any search vectors (since the search vectors must have the same dimensions as the indexed vectors). \n",
    "\n",
    "### Pre-requisites: \n",
    "You will need the sample data generated in the pervious notebook: `1_extract_sample_data.ipynb`\n",
    "\n",
    "**Author:** Leo Thomas - leo@developmentseed.org\\\n",
    "**Last updated:** 2023/06/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70370784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb73a4",
   "metadata": {},
   "source": [
    "### 1.0. Define data source and collect embedding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97896b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath(\"./one_percent_data_sample\")\n",
    "EMBEDDINGS_DIR = os.path.join(DATA_DIR, \"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f096d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_files = [\n",
    "    os.path.join(EMBEDDINGS_DIR, file) \n",
    "    for file in os.listdir(EMBEDDINGS_DIR) \n",
    "    if os.path.isfile(os.path.join(EMBEDDINGS_DIR, file))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f510b8",
   "metadata": {},
   "source": [
    "### 2.0. Define training sample size\n",
    "The PCA matrix needs to hold all vectors in memory while training. In order to manage memory capacity we will randomly select a subset of the vectors to train the PCA matrix. It's very import that the vectors be randomly selected such that they have the same distribution as the overall vectors to which the PCA dimensionality reduction will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17caa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_TRAINING_SAMPLE_SIZE = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb77051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cf1f13c2524c79a98a44bb98bc16ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 2min 28s, total: 2min 50s\n",
      "Wall time: 4min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3602912, 1280)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_set = []\n",
    "\n",
    "for embedding_file in tqdm(embeddings_files): \n",
    "\n",
    "    embeddings = np.load(embedding_file)\n",
    "    \n",
    "    # select random subset by generated random indexes to select\n",
    "    rand_indexes = np.random.randint(\n",
    "        low=0, high=len(embeddings), size=int(PCA_TRAINING_SAMPLE_SIZE * len(embeddings))\n",
    "    )\n",
    "    subset = embeddings[rand_indexes]\n",
    "    \n",
    "    # use native python list for append to the training set \n",
    "    # (since you can't append to numpy arrays inplace)\n",
    "    training_set.extend(list(subset))\n",
    "\n",
    "# convert training set to numpy array for compatibility\n",
    "# with the faiss PCA module\n",
    "training_set = np.array(training_set)\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38dff55",
   "metadata": {},
   "source": [
    "### 3.0. Train the PCA matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e522f2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 7min 16s, total: 8min 17s\n",
      "Wall time: 22min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define PCA matrix\n",
    "mat = faiss.PCAMatrix(1280, 256)\n",
    "\n",
    "# train\n",
    "mat.train(training_set)\n",
    "\n",
    "# write to file\n",
    "faiss.write_VectorTransform(mat, \"1280_to_256_dimensionality_reduction.pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57dc42e",
   "metadata": {},
   "source": [
    "### 4.0. Apply the trained PCA matrix to the entire embedding set and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c642ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCED_EMBEDDINGS_DIR = os.path.join(DATA_DIR, \"reduced_embeddings\")\n",
    "if not os.path.exists(REDUCED_EMBEDDINGS_DIR):\n",
    "    os.mkdir(REDUCED_EMBEDDINGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76850f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72f00c5af7a492cb6b8bb703644ed8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 55.9 s, total: 3min 11s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read trained PCA matrix from file\n",
    "pca_matrix = faiss.read_VectorTransform(\"1280_to_256_dimensionality_reduction.pca\")\n",
    "\n",
    "for embedding_file in tqdm(embeddings_files): \n",
    "    \n",
    "    embeddings = np.load(embedding_file)\n",
    "    \n",
    "    # apply the dimensionality reduction using the PCA matrix\n",
    "    reduced_embeddings = pca_matrix.apply(embeddings)\n",
    "    \n",
    "    # write the reduced embeddings to disk\n",
    "    filename = embedding_file.split(\"/\")[-1]\n",
    "    reduced_embeddings_filepath = os.path.join(REDUCED_EMBEDDINGS_DIR, filename)\n",
    "    np.save(reduced_embeddings_filepath, reduced_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioacoustics",
   "language": "python",
   "name": "bioacoustics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
